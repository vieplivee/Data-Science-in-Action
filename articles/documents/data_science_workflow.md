# Data science workflow

----
Data science consists of two parts: skills, and domain expertise. Skills are the means required for you to do any data science work, and domain expertise is the driving force that guides the actually data science workflow. Without domain expertise we won't reach fruitful results no matter how skillful we are.
Typically, the data science workflow is guided by domain expertise, and involves Data Munging, Data Mining, and Delivery of Actionable Insights. It does not run in strictly linear fashion. For example, we will go back to do some more data cleansing whenever a data bug is identified.

Now let's take a deeper look at Data Munging. The purpose of Data munging is  to prepare data for data mining. First, we want to extract, transform, and load data. This includes acquiring data from both internal and external data sources. After ETL, we want do integrate the data into a meaningful format. Throughout this process, we want to watch out for data discrepencies. This often means things like missing data, deduplicate data, or incorrect data. As you can imagine, an important step of data munging is to cleanse the data.

After Data Munging, the next step in a typical data science workflow is Data Mining. Data Exploration is usually the first thing to do. By exploring and examining the data, we obtain deeper understanding of the data to be ready for the next steps, such as feature construction. Data Mining also includes performing machine learning. Typically we want to construct new features, remove features if they are redundant or unapplicable, and choose some suitable algorithms to perform. Model Evaluation is an important and necessary step following Machine Learning. Here we check the validity of the model, and identify the best performing model that fits our data.
Now let's look at the skills. First of all, we need to program, a lot. For that purpose we need to have a tool set for programming, such as Python, R, or SQL. If we think of programming as the "hard" skills of a data scientist, then the "soft" skills would be statistical knowledge, things like, probability theory, descriptive statistics, and machine learning. I also want to mention visualization skills here, for in my opinion, this is one of the most important skills on which the success of a data science project will be depending on, for reasons I'll mention in a short moment.<img src="../../graphs/data_science_skills_domain.png" width="500"/>
If we put the skills and the domain expertise side by side, we see an almost one-to-one correspondence between them. Typically, we use programming skills to munge data, and we need to be weaponed with statistical knowledge in order to mine data. After all the hard work is done, the success criteria of a piece of data science work is most likely measured by its delivery of the so-called actionable insights - the insights that's not only immediately understandable to the business users who asked you to do this project to begin with, but also "actionable" in the sense that the business users will see direct actions that they'll need to take to make better decisions.
This procedure is somewhat similar to a doctor prescribing medicine to a patient. The doctor would start with asking questions, followed by gathering insights of the patient's condition through experience and lab test. As a result of this clinical visit, the patient typically expect to obtain a prescription from the doctor which will be used for the patient to take actions, which is, taking the medicine. This is why the process of obtaining actionable insights is often called "prescriptive analysis".
Now thinking about it, the delivery of actionable insights will be dependent on our understanding of the business domain, the business question, and most of the time, it is carried out through effective and meaningful data visualization. 

----

- Created: 03.24.2015
- Last Updated: 03.26.2015